---
title: "Variance, Covariance, Correlation"
output:
  pdf_document: default
  html_notebook: default
  html_document: default
---

# Variance, Covariance, Correlation

##Variance

Variance is a simple way of quantifying the amount of variability around a mean in a dataset. It's calculated simply as the sum of squares of the data, after subtracting the mean of the data. Or in linear algebra:

$$Variance = \frac{(x - \bar{x})^T (x - \bar{x})}{df}$$
And df is usually N-1

Let's examine this with some data in R, will use the dataset "trees"

```{r}
head(trees)
```


Let's calculate the variance of the tree girth.
```{r}
girthAnom = trees[,1]-mean(trees[,1])
girthVar = t(girthAnom)%*%girthAnom / (length(girthAnom)-1)
print(girthVar)
#check it
var(trees[,1])
```
What are the units of this variance?

What if you wanted it in the units of the dataset?

###Standard deviation.
Yep, standard deviation is just the square root of the varaince

OK, let's visualize what we just did

```{r}
library(ggplot2)
mean.girth = mean(trees$Girth)
trees = trees
trees$girth.anom = trees$Girth-mean.girth
head(trees)
ggplot(trees,aes(x=1:nrow(trees),y=girth.anom))+geom_bar(stat="identity")

```

##Standard deviation
##Covariance

OK, now that you understand what variance is, what do you think covariance is?

We'll use trees again, and calculate the covariance of height and volume. First let's make a scatter plot.

```{r}
ggplot(trees,aes(x=Height, y=Volume)) + geom_point() 
```
OK, there's clearly a relationship, so let's calculate the covariance. The equation for covariance is just like variance: 

$$Covariance = \frac{(x - \bar{x})^T (y - \bar{y})}{df}$$
and df is typically N-1

In R:

```{r}
height.anom = trees$Height - mean(trees$Height)
volume.anom = trees$Volume - mean(trees$Volume)
covarianceHV = t(height.anom) %*% volume.anom / (length(trees$Volume)-1)
print(covarianceHV)
#check it 
cov(trees$Height,trees$Volume)

```
What are the units on that number?

OK, let's compare that with covariance between Girth and Volume:



```{r}
girth.anom = trees$Girth - mean(trees$Girth)
volume.anom = trees$Volume - mean(trees$Volume)
covarianceGV = t(girth.anom) %*% volume.anom / (length(trees$Volume)-1)
print(covarianceGV)
#check it
cov(girth.anom,volume.anom)

```

So which of the two datasets is more related to volume?


##Correlation
Right. So covariance is a useful concept, but difficult to interpret on its own, and not particularly valuable for comparisons between unlike variables. Perhaps we should *normalize* that that number by something useful.

Suggestions?

Seems like if we normalize it by the highest possible amount of covariance, that would reveal the relative amount covariance.

The maximum possible covariance occurs when two vectors are linear transformations of themselves. If the identical, that's equivalent to variance. If not, the maximum covariance is 

$$\sqrt{var_x * var_y}$$

So the relative amount of the highest covariance is

$$\rho = \frac{cov}{\sqrt{var_x * var_y}}$$
This is called *correlation*, and $\rho$ is the correlation coefficient.

Let's calculate $\rho$ for each of our covariances above and see which is higher.

```{r}
maxCovHV = sqrt(t(height.anom)%*%height.anom / (length(height.anom)-1) * t(volume.anom)%*%(volume.anom) / (length(height.anom)-1))
maxCovGV =  sqrt(t(girth.anom)%*%girth.anom / (length(height.anom)-1) * t(volume.anom)%*%(volume.anom)  / (length(height.anom)-1))

rhoHV = covarianceHV/maxCovHV
rhoGV = covarianceGV/maxCovGV

print(rhoHV)
print(rhoGV)
```
Let's check our answer using the built in cor() function:
```{r}
print(cor(trees$Height,trees$Volume))
print(cor(trees$Girth,trees$Volume))

```
Phew. It worked.

##Fraction of Variance. 

So if we square $\rho$, get:

$$\rho^2 = \frac{cov^2}{var_x*var_y}$$
Which compares the covariance, to the total variance in both datasets. This is why it's reasonable to think of $\rho^2$ as the fraction of total variance explained by the covariance.


##Significance testing
Now that we have a number that's comparable between multiple datasets, we'd like to have some idea if our correlation coefficients are something special. Is a value of 0.23 good? How about -0.5? Which means that there's  a "real" correlation?

Let's think about his in terms of a null hypothesis. If we set our null hypothesis to be: 

"The correlation between these two datasets is less extreme than what is produced by two random datasets 95% of the time"

If we can show that our value is higher than that 95% confidence level, then we reject the null hypothesis, and call it significant at the 95% level.

So let's generate a whole bunch of random data to see what that looks like. 

```{r}
r=c()#Initialize a spot for our random calculations to go
for(i in 1:1000){#lets do this a thousand times
  rd1 = rnorm(100)#simulate data each time through the loop
  rd2 = rnorm(100)#twice
  r[i] = cor(rd1,rd2)#correlate and store the values
}
#now let's plot it
ggplot()+geom_histogram(aes(x=r,y=..density..)) #Let's look at my favorite plot


```

And we see that this looks something like a normal distribution, but is sensitive to the number of observations. This is modeled well with a Student's T distribution, and you'll go through this in your lab. I'll do an example here with with a different test we want to use a t-distribution for.


##Probability testing example, difference of means testing.
To do this, we'll take a look at the Beavers dataset.

We're interested in the probability that Beaver1's mean body temperature is less than 37 degrees.

```{r}
#Lets take a look at our observations
ggplot(beaver1)+geom_histogram(aes(x=temp,y=..density..))+geom_vline(xintercept = 37,color="red")
```
Student T distributions, by definition, always have a mean of zero. Let's make our data have a mean of zero and a standard deviation of 1, and then we can compare it to a Students' T.


```{r}
#Lets take a look at our observations
myplot = ggplot(beaver1)+geom_histogram(aes(x=(temp-mean(temp))/(sd(temp)/sqrt(length(beaver1$temp))),y=..density..))+geom_vline(aes(xintercept = (37-mean(beaver1$temp))/(sd(beaver1$temp)/sqrt(length(beaver1$temp))),color="red"))
#and let's add a tdistribution
td = dt(seq(-3,3,by=.1),df=length(beaver1$temp)-2)
ndf = data.frame(x=seq(-3,3,by=.1),y=td)
yplot+geom_area(data=ndf,aes(x=x,y=y),fill="red",alpha=0.5)
```


Again, we're no longer comparing to see if the mean temperature is less than $37$, we've converted $37$ into T-distribution space by subtracting the mean and dividing by the standard deviation.

```{r}
#Tstat = (37-mean(beaver1$temp))/(sd(beaver1$temp)/sqrt(length(beaver1$temp)))
print(Tstat)
```


So now we want to see how our T-distribution compares to our "T-stat"
So we'll use the cumulative probability function, to see the probability that the T-dsitribution with the appropriate df is less than our T-stat
```{r}
pval = pt(Tstat,df = length(beaver1$temp)-2)
print(pval)
```

OK, but what if we wanted to estimate how well we know our mean, or more specifically, the probability that our mean is more than 0.1 degrees different than our measured mean?

Now we're interested in the probability that the mean is:

*less than 36.83 degrees or greater than 36.89 degrees*

Graphically, this might look like this:
```{r}
tstatHi = 0.03/(sd(beaver1$temp)/sqrt(length(beaver1$temp)))
tstatLo = -0.03/(sd(beaver1$temp)/sqrt(length(beaver1$temp)))

x1 = seq(-4,4,length.out = 50)
d1 = dt(x1,df=length(beaver1-2))
x2 = seq(tstatHi,4,length.out = 50)
d2 = dt(x2,df=length(beaver1-2))
x3 = seq(-4,tstatLo,length.out = 50)
d3 = dt(x3,df=length(beaver1-2))

plotdf = data.frame(x1,x2,x3,d1,d2,d3)
ggplot(plotdf)+geom_area(aes(x1,d1),fill = "black")+geom_area(aes(x2,d2),fill="red",alpha=0.5)+geom_area(aes(x3,d3),fill="red",alpha=0.5)
```


We can test this with a T-test too, and because the T-distribution is symmetrical, we can just look at the cumulative probability on the left side, and double it:

```{r}
pt(tstatLo,df=length(beaver1$temp))*2
```
Or if we were generalizing, and about the probability that the value was *x* more extreme than the mean, we could say:
```{r}
x=tstatHi#x can be positive or negative now!

pt(-abs(x),df=length(beaver1$temp))*2

```

##Monte-carlo methods
Let's imagine a different way to do this analysis. What if instead using a probability distribution, we wanted to just simulate the mean 1000 times, and see the probability that it's outside the range of the we defined above that way?

Using random number generators to make large simulations of data, and then looking at those results is a class of methods called "Monte Carlo" methods.

We'll use rnorm() to generate a our beaver temperature data 1000 times. 

```{r}
bmean = mean(beaver1$temp)
bsd = sd(beaver1$temp)
nits = 1000
simBeaverMean = matrix(NA,nits,1)
for(i in 1:nits){#for every iteration
  simBeaver = rnorm(n = length(beaver1$temp),mean = bmean,sd=bsd)
  simBeaverMean[i] = mean(simBeaver)
  
  }


```
OK, now we've calculated 1000 random means, let's see what that distribution looks like:

```{r}
hiTemp = mean(beaver1$temp)+0.03
loTemp = mean(beaver1$temp)-0.03
ggplot()+geom_histogram(aes(x=simBeaverMean))+geom_vline(aes(xintercept=loTemp))+geom_vline(aes(xintercept=hiTemp))
```

And now let's figure what where are 95% confidence interval is...
```{r}
quantile(simBeaverMean,c(0.025,0.975))
```

Or, what probabilities our hi or low temperature ranges correspond to

```{r}
beaverCDF = ecdf(simBeaverMean)
ourProbs = beaverCDF(c(loTemp,hiTemp))
totalExtremeProb = ourProbs[1]+(1-ourProbs[2])
```




##Side note - functions with multiple outputs.
One last thing. It's often useful to calculate values with multiple, and sometimes different kinds of outputs. But R only lets you export one thing from a function, and then it ends. Do this, you should take advantage of R's most general data type, the *list*. Think of a list as a bucket of any other kind of data with a name. Here's an example:

Write a function that you input a vector, and it returns both the mean, standard deviation and the input vector.

```{r}
awesome = function(vec){
  out = list()#initialize my list
  out$mean = mean(vec)#calculate and store the mean
  out$sd = sd(vec)#again for sd
  out$input = vec#and store the vector.
  
  return(out)
}


my.out = awesome(rnorm(100)) #let's try with 100 random datapoints
print(my.out$mean)
print(my.out$input)



```





