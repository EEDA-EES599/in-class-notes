---
title: "R Notebook"
output:
  pdf_document: default
  html_notebook: default
  html_document: default
---

#Principal Components Analysis

Let's look for the modes of variability in this diamonds dataset:

```{r}
library(ggplot2)
head(diamonds)
#let's make a big plot avoiding the three rows that aren't data
plot(diamonds[1:100,-c(2,3,4)])

Xraw = diamonds[1:1000,-c(2,3,4)]

```


OK, now we need to calculate a covariance matrix 
```{r}
#subtract the mean from each column. We can do this easily with scale()

#and use scale() to subtract that mean.
X = scale (Xraw, scale=TRUE)
#what does the scale=FALSE do?
X2= scale (Xraw, scale=FALSE)

head(X)
head(X2)

#let's calculate our covariance matrix:
coMat  = (t(X)%*%X)/ (nrow(X)-1)
head(coMat)
#compare that to a correlation matrix
head(cor(Xraw))
```
Aha, so a scaled covariance matrix is identical to a correlation matrix. Good to know.

So let's do PCA on the correlation matrix, since the units are different in our column

```{r}
#Now that we have our covariance/correlation matrix, we can use svd to find the eigen values and vectors:
out = svd(coMat)
names(out)
```
OK, remember our SVD formulation

$A = EDE^T$

Where A is our covariance matrix, E is our eigenvectors, and D is our Eigenvalues?
svd names E=U, and $E^T$ = V

We can also think our eigen matrix as our new covariance matrix, with zeros off the diagonal.

```{r}
print(out$d)#svd returns only, the diagonal.

#so our total covariance is just the sum of out$d
sum(out$d)

#and the fraction of variance in each is
out$d/sum(out$d)
```
OK, so now we have 7 indepenent modes of variability, that explain all the variance, independently.

But can we take our data, and "project it" onto this new coordinate system. Yes we can!


```{r}
PCs = (X %*% out$v) #this is our projection of our data on the eigen vectors. We call this the PC score. 
dim(PCs)
cov(PCs)#did it work? If so each PC score should be uncorrelated with all the others. 
```

OK, great. How can we explore these data?
```{r}
df = cbind(as.data.frame(PCs),diamonds[1:nrow(X),]) #let's make a big data frame with the original data, and our PCs
df2 = cbind(as.data.frame(out$v),names(diamonds[,-c(2,3,4)])) #and a second with just the eigenvectors
names(df2)[8]="names"
arrowScale=6 #lets setup the length of our arrows as a variable
ggplot(df, aes(x = V1, y= V2))+#and make an awesome plot, we're going to compare PC1 and 2
  geom_point(aes(colour=cut),size=6)+ #first just plotting points and colouring them by the cut
  geom_hline(yintercept=0)+geom_vline(xintercept=0)+#then we'll plot some 0 lines for our reference
  coord_fixed(xlim=c(-5,5),ylim=c(-5,5))+#and set the scale
  geom_segment(data=df2,aes(x=0,y=0,xend = V1*arrowScale, yend = V2*arrowScale))+#Now we'll add lines that correspond to our eigen vectors
  geom_text(data=df2,aes(x = V1*arrowScale, y = V2*arrowScale,label=names) )#and label those lines

```


#Mapping!
To map, we will use the ggmap package which integrates nicely with ggplot

```{r}
#install.packages("ggmap")
library(ggmap)
```
And for fun, let's take a look at a spatial database of vehicle accidents...
```{r}
mydata = read.csv("vehicle-accidents.csv")
head(mydata)

```
And reformat the data slightly to make it map better

```{r}
mydata$State = as.character(mydata$State)
mydata$MV.Number = as.numeric(mydata$MV.Number)
```


And get rid of the rows that will be a problem to map...
```{r}
#get rid of HI, AK, and the whole US total
mydata = mydata[mydata$State != "Alaska",]
mydata = mydata[mydata$State != "Hawaii",]
mydata = mydata[mydata$State != "USA",]
```


ggmap does a lot of awesome things. One them is the geocode() function
```{r}
NAU = geocode("Northern Arizona University")
print(c(NAU$lat,NAU$lon))
```
Notice that there are no coordinates in our database. Let's use geocode to get coordinates for our map:

```{r}
#get latitude and longitude
for (i in 1:nrow(mydata)){
  latlon = geocode(mydata$State[i])
  mydata$lon[i] = as.numeric(latlon[1])
  mydata$lat[i] = as.numeric(latlon[2])
}
```

Awesome. We're ready to make a map. ggmap works by ggplotting on a base map. There are a number of ways to make base map. One is by creating a bounding box:
```{r}
bb = make_bbox(lat = mydata$lat, lon = mydata$lon, f = .3)
map = get_map(location = bb)
```


```{r}
names(mydata)[3]="collisions"#let's rename that column

ggmap(map)+geom_point(data=mydata,aes(x = lon, y = lat, size = collisions),colour ="red",alpha=.6)+scale_colour_continuous(low="white",high="red")

```
```{r}
# find a reasonable spatial extent
head(crime)

qmap('houston', zoom = 13)
# only violent crimes
violent_crimes <- subset(crime,offense != "auto theft" & offense != "theft" & offense != "burglary")
# order violent crimes
violent_crimes$offense <- factor(violent_crimes$offense, levels =c("robbery", "aggravated assault","rape", "murder"))
# restrict to downtown
violent_crimes <- subset(violent_crimes,-95.39681 <= lon & lon <= -95.34188 & 29.73631 <= lat & lat <= 29.78400)
```


```{r}
theme_set(theme_bw(16))
HoustonMap <- qmap("houston", zoom = 14,color = "bw", legend = "topleft")
HoustonMap + geom_point(aes(x = lon, y = lat,colour = offense, size = offense),data = violent_crimes)



HoustonMap +stat_bin2d(aes(x = lon, y = lat, colour = offense,fill = offense),size = .5, bins = 30, alpha = 1/2,data = violent_crimes)
```

```{r}
houston <- get_map("houston", zoom = 14)
HoustonMap <- ggmap(houston,extent = "device", legend = "topleft")
HoustonMap +stat_density2d(aes(x = lon, y = lat, fill = ..level..,alpha = ..level..),size = 2, bins = 4, data = violent_crimes,geom = "polygon")
```




